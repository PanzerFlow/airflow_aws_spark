[2022-02-05 19:40:14,633] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: beginner_batch_processing.upload_spark_script_to_s3 manual__2022-02-05T19:40:00.731743+00:00 [queued]>
[2022-02-05 19:40:14,649] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: beginner_batch_processing.upload_spark_script_to_s3 manual__2022-02-05T19:40:00.731743+00:00 [queued]>
[2022-02-05 19:40:14,650] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2022-02-05 19:40:14,650] {taskinstance.py:1239} INFO - Starting attempt 1 of 3
[2022-02-05 19:40:14,650] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2022-02-05 19:40:14,670] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): upload_spark_script_to_s3> on 2022-02-05 19:40:00.731743+00:00
[2022-02-05 19:40:14,676] {standard_task_runner.py:52} INFO - Started process 1032 to run task
[2022-02-05 19:40:14,682] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'beginner_batch_processing', 'upload_spark_script_to_s3', 'manual__2022-02-05T19:40:00.731743+00:00', '--job-id', '25', '--raw', '--subdir', 'DAGS_FOLDER/beginner_batch_processing.py', '--cfg-path', '/tmp/tmpx5b8b9lo', '--error-file', '/tmp/tmprbua7nzw']
[2022-02-05 19:40:14,683] {standard_task_runner.py:77} INFO - Job 25: Subtask upload_spark_script_to_s3
[2022-02-05 19:40:14,759] {logging_mixin.py:109} INFO - Running <TaskInstance: beginner_batch_processing.upload_spark_script_to_s3 manual__2022-02-05T19:40:00.731743+00:00 [running]> on host 3479a57d1d64
[2022-02-05 19:40:14,896] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=beginner_batch_processing
AIRFLOW_CTX_TASK_ID=upload_spark_script_to_s3
AIRFLOW_CTX_EXECUTION_DATE=2022-02-05T19:40:00.731743+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2022-02-05T19:40:00.731743+00:00
[2022-02-05 19:40:14,898] {base_aws.py:401} INFO - Airflow Connection: aws_conn_id=aws_default
[2022-02-05 19:40:14,914] {base_aws.py:177} INFO - Credentials retrieved from login
[2022-02-05 19:40:14,914] {base_aws.py:88} INFO - Retrieving region_name from Connection.extra_config['region_name']
[2022-02-05 19:40:14,915] {base_aws.py:93} INFO - Creating session with aws_access_key_id=AKIA53YL7CVELHKGVOP4 region_name=us-east-1
[2022-02-05 19:40:14,941] {base_aws.py:168} INFO - role_arn is None
[2022-02-05 19:40:14,943] {warnings.py:110} WARNING - /home/***/.local/lib/python3.7/site-packages/***/providers/amazon/aws/hooks/base_aws.py:494: DeprecationWarning: client_type is deprecated. Set client_type from class attribute.
  return self.get_client_type(self.client_type, region_name=self.region_name)

[2022-02-05 19:40:15,485] {python.py:175} INFO - Done. Returned value was: None
[2022-02-05 19:40:15,513] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=beginner_batch_processing, task_id=upload_spark_script_to_s3, execution_date=20220205T194000, start_date=20220205T194014, end_date=20220205T194015
[2022-02-05 19:40:15,577] {local_task_job.py:154} INFO - Task exited with return code 0
[2022-02-05 19:40:15,670] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
