[2022-02-05 19:32:08,252] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: beginner_batch_processing.upload_spark_script_to_s3 manual__2022-02-05T19:31:57.468572+00:00 [queued]>
[2022-02-05 19:32:08,260] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: beginner_batch_processing.upload_spark_script_to_s3 manual__2022-02-05T19:31:57.468572+00:00 [queued]>
[2022-02-05 19:32:08,261] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2022-02-05 19:32:08,261] {taskinstance.py:1239} INFO - Starting attempt 1 of 3
[2022-02-05 19:32:08,261] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2022-02-05 19:32:08,272] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): upload_spark_script_to_s3> on 2022-02-05 19:31:57.468572+00:00
[2022-02-05 19:32:08,276] {standard_task_runner.py:52} INFO - Started process 607 to run task
[2022-02-05 19:32:08,280] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'beginner_batch_processing', 'upload_spark_script_to_s3', 'manual__2022-02-05T19:31:57.468572+00:00', '--job-id', '15', '--raw', '--subdir', 'DAGS_FOLDER/beginner_batch_processing.py', '--cfg-path', '/tmp/tmpodp8cwhq', '--error-file', '/tmp/tmp81swv2wn']
[2022-02-05 19:32:08,280] {standard_task_runner.py:77} INFO - Job 15: Subtask upload_spark_script_to_s3
[2022-02-05 19:32:08,331] {logging_mixin.py:109} INFO - Running <TaskInstance: beginner_batch_processing.upload_spark_script_to_s3 manual__2022-02-05T19:31:57.468572+00:00 [running]> on host 3479a57d1d64
[2022-02-05 19:32:08,380] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=beginner_batch_processing
AIRFLOW_CTX_TASK_ID=upload_spark_script_to_s3
AIRFLOW_CTX_EXECUTION_DATE=2022-02-05T19:31:57.468572+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2022-02-05T19:31:57.468572+00:00
[2022-02-05 19:32:08,381] {base_aws.py:401} INFO - Airflow Connection: aws_conn_id=aws_default
[2022-02-05 19:32:08,390] {base_aws.py:177} INFO - Credentials retrieved from login
[2022-02-05 19:32:08,390] {base_aws.py:88} INFO - Retrieving region_name from Connection.extra_config['region_name']
[2022-02-05 19:32:08,391] {base_aws.py:93} INFO - Creating session with aws_access_key_id=AKIA53YL7CVELHKGVOP4 region_name=us-east-1
[2022-02-05 19:32:08,403] {base_aws.py:168} INFO - role_arn is None
[2022-02-05 19:32:08,404] {warnings.py:110} WARNING - /home/***/.local/lib/python3.7/site-packages/***/providers/amazon/aws/hooks/base_aws.py:494: DeprecationWarning: client_type is deprecated. Set client_type from class attribute.
  return self.get_client_type(self.client_type, region_name=self.region_name)

[2022-02-05 19:32:08,995] {python.py:175} INFO - Done. Returned value was: None
[2022-02-05 19:32:09,005] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=beginner_batch_processing, task_id=upload_spark_script_to_s3, execution_date=20220205T193157, start_date=20220205T193208, end_date=20220205T193209
[2022-02-05 19:32:09,055] {local_task_job.py:154} INFO - Task exited with return code 0
[2022-02-05 19:32:09,090] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
